
Multiple Features:

  n = number of features
  x(i) = vector of features of row i as column
  x(i)(j) = j denotes the vector component

  univariate Regression, one variable
  Now Multivariate Regression:

    h(x(i)) = t0 + t(j)*x(j) .. t(j+1)*x(j+1) = t^T*x

    x = [x0; x1; x2; .. xn]
    t = [t0; t1; t2; ..tn]

Multivariate Regression and Gradient Descant:
  
  -

Feature Scaling:

  scale the features to fit to each other e.g.
  divide the current feature by the maximum number
  it can take

   -> range from -3 to 3 or -1/3 to 1/3 Okay

  Mean Normalization:

    size of houses = 50-2000 
      (all samples average 1000)
      min_val = 50 
      max_val = 2000
      range_min_to_max = 1950
    x1 = (size - average) / range_min_to_max

  Gradient Descant will be much faster with the mean normalization


Gradient Descant works correctly:
  
  min J(0), after each iteration it should
  minimize, if not something is wrong with alpha

  10^-3 as a threshold when you are satisfied for one
  iteration

  alpha too small = slow convergence
  alpha too large = might not decrease

  start at 0.001, 3x0.003 
           0.01 3x0.03
           0.1 3x0.3
           1.0

Features and Polynominal Regression:
  take one feature and make it polynominal (must be well scaled)


Normal Equation:
  
  Feature scaling not important,
  Some Matrix product voodoo

  GD: 
    choosing alpha, 
    needs many iterations
    works well when n is large
    n = from 10000
  NE:
    no alpha
    no iterations
    n x n matrix, inverse computation: O(n^3)
      n = upto 10000, after that, inverses get slow

